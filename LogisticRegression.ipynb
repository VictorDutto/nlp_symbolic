{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97532498-ca3b-4acb-abc4-3e3df3bccd26",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f87543d-7238-4882-9b80-2cf69bf48897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011eb56c-8ab5-4d89-bf12-888b07b2f6d6",
   "metadata": {},
   "source": [
    "### Importing HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bb2c3c-97d1-4484-94cf-22c15128b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install contractions\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5192592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f03088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/token/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/home/token/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('imdb', split='train')\n",
    "test_dataset = load_dataset('imdb', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec7fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bdc484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359e9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94860bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document):\n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    #special_char_pattern = re.compile(r'([{.(-)}])')\n",
    "    #document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    #document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "381c90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pre_process_corpus(train_dataset['text'])\n",
    "x_train_preprocessed = []\n",
    "\n",
    "for elt in x_train:\n",
    "    x_train_preprocessed.append(np.array(elt.split(\" \")))\n",
    "    \n",
    "y_train = train_dataset['label']\n",
    "\n",
    "\n",
    "\n",
    "x_test_preprocessed = []\n",
    "\n",
    "x_test = pre_process_corpus(test_dataset['text'])\n",
    "y_test = test_dataset['label']\n",
    "\n",
    "for elt in x_test:\n",
    "    x_test_preprocessed.append(np.array(elt.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "54e8b334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell' 'high' 'is' 'a' 'cartoon' 'comedy.' 'it' 'ran' 'at' 'the'\n",
      " 'same' 'time' 'as' 'some' 'other' 'programs' 'about' 'school' 'life,'\n",
      " 'such' 'as' '\"teachers\".' 'my' '35' 'years' 'in' 'the' 'teaching'\n",
      " 'profession' 'lead' 'me' 'to' 'believe' 'that' 'bromwell' \"high's\"\n",
      " 'satire' 'is' 'much' 'closer' 'to' 'reality' 'than' 'is' '\"teachers\".'\n",
      " 'the' 'scramble' 'to' 'survive' 'financially,' 'the' 'insightful'\n",
      " 'students' 'who' 'can' 'see' 'right' 'through' 'their' 'pathetic'\n",
      " \"teachers'\" 'pomp,' 'the' 'pettiness' 'of' 'the' 'whole' 'situation,'\n",
      " 'all' 'remind' 'me' 'of' 'the' 'schools' 'i' 'knew' 'and' 'their'\n",
      " 'students.' 'when' 'i' 'saw' 'the' 'episode' 'in' 'which' 'a' 'student'\n",
      " 'repeatedly' 'tried' 'to' 'burn' 'down' 'the' 'school,' 'i' 'immediately'\n",
      " 'recalled' '.........' 'at' '..........' 'high.' 'a' 'classic' 'line:'\n",
      " 'inspector:' 'I' 'am' 'here' 'to' 'sack' 'one' 'of' 'your' 'teachers.'\n",
      " 'student:' 'welcome' 'to' 'bromwell' 'high.' 'i' 'expect' 'that' 'many'\n",
      " 'adults' 'of' 'my' 'age' 'think' 'that' 'bromwell' 'high' 'is' 'far'\n",
      " 'fetched.' 'what' 'a' 'pity' 'that' 'it' 'is' 'not!']\n",
      "['i' 'went' 'and' 'saw' 'this' 'movie' 'last' 'night' 'after' 'being'\n",
      " 'coaxed' 'to' 'by' 'a' 'few' 'friends' 'of' 'mine.' 'I' 'will' 'admit'\n",
      " 'that' 'i' 'was' 'reluctant' 'to' 'see' 'it' 'because' 'from' 'what' 'i'\n",
      " 'knew' 'of' 'ashton' 'kutcher' 'he' 'was' 'only' 'able' 'to' 'do'\n",
      " 'comedy.' 'i' 'was' 'wrong.' 'kutcher' 'played' 'the' 'character' 'of'\n",
      " 'jake' 'fischer' 'very' 'well,' 'and' 'kevin' 'costner' 'played' 'ben'\n",
      " 'randall' 'with' 'such' 'professionalism.' 'the' 'sign' 'of' 'a' 'good'\n",
      " 'movie' 'is' 'that' 'it' 'can' 'toy' 'with' 'our' 'emotions.' 'this'\n",
      " 'one' 'did' 'exactly' 'that.' 'the' 'entire' 'theater' '(which' 'was'\n",
      " 'sold' 'out)' 'was' 'overcome' 'by' 'laughter' 'during' 'the' 'first'\n",
      " 'half' 'of' 'the' 'movie,' 'and' 'were' 'moved' 'to' 'tears' 'during'\n",
      " 'the' 'second' 'half.' 'while' 'exiting' 'the' 'theater' 'i' 'not' 'only'\n",
      " 'saw' 'many' 'women' 'in' 'tears,' 'but' 'many' 'full' 'grown' 'men' 'as'\n",
      " 'well,' 'trying' 'desperately' 'not' 'to' 'let' 'anyone' 'see' 'them'\n",
      " 'crying.' 'this' 'movie' 'was' 'great,' 'and' 'i' 'suggest' 'that' 'you'\n",
      " 'go' 'see' 'it' 'before' 'you' 'judge.']\n"
     ]
    }
   ],
   "source": [
    "print(x_train_preprocessed[0])\n",
    "print(x_test_preprocessed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31a9af",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5351241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb30bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_lexicon(path):\n",
    "    data = pd.read_csv(path, sep='\\t', names=[0, 1, 2, 3])\n",
    "    df = pd.DataFrame()\n",
    "    df['token'] = data[0]\n",
    "    df['sentiment'] = data[1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853b85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_no_appear(review) -> int:\n",
    "    if \"no\" in review:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "755e64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_first_and_second_pro(review) -> int:\n",
    "    count = 0\n",
    "    for word in review:\n",
    "        if word in [\"I\", \"i\", \"you\", \"yours\"]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10640c",
   "metadata": {},
   "source": [
    "Let's search for all the words finishing with !\n",
    "\n",
    "As it would make no sense that the ! character appears before the end of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8e86cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_exclamation_appear(review) -> int:\n",
    "    if \"!\" in review:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6d0be",
   "metadata": {},
   "source": [
    "it appears no words ends with ! in the training set\n",
    "\n",
    "Thus, there is no need to check for ! in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d10a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_word_count_in_doc(review):\n",
    "    return np.log(len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lexicon(lexicon, word_index):\n",
    "    # only the int value of the words present in index\n",
    "    filtered_lexicon, i = pd.DataFrame([], columns=['token_int', 'token_string', 'sentiment']), 0\n",
    "    for index in range(lexicon.shape[0]):\n",
    "        token = lexicon['token'][index]\n",
    "        if token in word_index.keys():\n",
    "            mapped_token = word_index[token]\n",
    "            filtered_lexicon.loc[i] = [mapped_token, lexicon.token[index], lexicon.sentiment[index]]\n",
    "            i += 1\n",
    "    return filtered_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6ffbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexicon(lexicon):\n",
    "    positive_df = lexicon[lexicon.sentiment > 0]\n",
    "    positive_words = positive_df['token'].to_numpy().tolist()\n",
    "    negative_df = lexicon[lexicon.sentiment < 0]\n",
    "    negative_words = negative_df['token'].to_numpy().tolist()\n",
    "    return positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6058e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_words_pos(review, positive_words):\n",
    "    posi = np.isin(positive_words, review)\n",
    "    return sum(posi)\n",
    "\n",
    "def number_of_words_neg(review, negative_words):\n",
    "    nega = np.isin(review, negative_words)\n",
    "    return sum(nega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "58f8e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoRegression(x_train, y_train):\n",
    "    lexicon = import_lexicon(\"vader_lexicon.txt\")\n",
    "    positive_words, negative_words = split_lexicon(lexicon)\n",
    "    X_features = []\n",
    "    for review in tqdm(x_train):\n",
    "        feature = np.zeros(6)\n",
    "        feature[0] = does_no_appear(review)\n",
    "        feature[1] = does_exclamation_appear(review)\n",
    "        feature[2] = count_first_and_second_pro(review)\n",
    "        feature[3] = log_word_count_in_doc(review)\n",
    "        feature[4] = number_of_words_neg(review, negative_words)\n",
    "        feature[5] = number_of_words_pos(review, positive_words)\n",
    "\n",
    "        X_features.append(feature)\n",
    "    return np.asarray(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "848e5082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [13:37<00:00, 30.57it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_features = LoRegression(x_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "484e87c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 6)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35a65246",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c48cdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [14:49<00:00, 28.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#create X_test using loRegression to have a usable informations\n",
    "X_test_features = LoRegression(x_test_preprocessed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fc22a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 6) (25000,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_features)\n",
    "print(X_test_features.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1235a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69612"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3c4cbdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.70      0.70      0.70     12500\n",
      "     class 1       0.70      0.70      0.70     12500\n",
      "\n",
      "    accuracy                           0.70     25000\n",
      "   macro avg       0.70      0.70      0.70     25000\n",
      "weighted avg       0.70      0.70      0.70     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
