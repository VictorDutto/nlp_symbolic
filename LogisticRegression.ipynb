{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97532498-ca3b-4acb-abc4-3e3df3bccd26",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f87543d-7238-4882-9b80-2cf69bf48897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011eb56c-8ab5-4d89-bf12-888b07b2f6d6",
   "metadata": {},
   "source": [
    "### Importing HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bb2c3c-97d1-4484-94cf-22c15128b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/surenis/.local/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: dill in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (1.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (0.0.16)\n",
      "Requirement already satisfied: packaging in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: xxhash in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenis/.local/lib/python3.8/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/surenis/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/surenis/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/surenis/.local/lib/python3.8/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/surenis/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/surenis/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/surenis/.local/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: filelock in /home/surenis/.local/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/surenis/.local/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/surenis/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: contractions in /home/surenis/.local/lib/python3.8/site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/surenis/.local/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /home/surenis/.local/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in /home/surenis/.local/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/surenis/.local/lib/python3.8/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/surenis/.local/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install contractions\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5192592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f03088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/surenis/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/home/surenis/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('imdb', split='train')\n",
    "test_dataset = load_dataset('imdb', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94860bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text : str) -> str:\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text : str, remove_digits : bool=False) -> str:\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document : str) -> str:\n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    #special_char_pattern = re.compile(r'([{.(-)}])')\n",
    "    #document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    #document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381c90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pre_process_corpus(train_dataset['text'])\n",
    "x_train_preprocessed = []\n",
    "\n",
    "for elt in x_train:\n",
    "    x_train_preprocessed.append(np.array(elt.split(\" \")))\n",
    "    \n",
    "y_train = train_dataset['label']\n",
    "\n",
    "\n",
    "\n",
    "x_test_preprocessed = []\n",
    "\n",
    "x_test = pre_process_corpus(test_dataset['text'])\n",
    "for elt in x_test:\n",
    "    x_test_preprocessed.append(np.array(elt.split(\" \")))\n",
    "                            \n",
    "y_test = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e8b334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell' 'high' 'is' 'a' 'cartoon' 'comedy.' 'it' 'ran' 'at' 'the'\n",
      " 'same' 'time' 'as' 'some' 'other' 'programs' 'about' 'school' 'life,'\n",
      " 'such' 'as' '\"teachers\".' 'my' '35' 'years' 'in' 'the' 'teaching'\n",
      " 'profession' 'lead' 'me' 'to' 'believe' 'that' 'bromwell' \"high's\"\n",
      " 'satire' 'is' 'much' 'closer' 'to' 'reality' 'than' 'is' '\"teachers\".'\n",
      " 'the' 'scramble' 'to' 'survive' 'financially,' 'the' 'insightful'\n",
      " 'students' 'who' 'can' 'see' 'right' 'through' 'their' 'pathetic'\n",
      " \"teachers'\" 'pomp,' 'the' 'pettiness' 'of' 'the' 'whole' 'situation,'\n",
      " 'all' 'remind' 'me' 'of' 'the' 'schools' 'i' 'knew' 'and' 'their'\n",
      " 'students.' 'when' 'i' 'saw' 'the' 'episode' 'in' 'which' 'a' 'student'\n",
      " 'repeatedly' 'tried' 'to' 'burn' 'down' 'the' 'school,' 'i' 'immediately'\n",
      " 'recalled' '.........' 'at' '..........' 'high.' 'a' 'classic' 'line:'\n",
      " 'inspector:' 'I' 'am' 'here' 'to' 'sack' 'one' 'of' 'your' 'teachers.'\n",
      " 'student:' 'welcome' 'to' 'bromwell' 'high.' 'i' 'expect' 'that' 'many'\n",
      " 'adults' 'of' 'my' 'age' 'think' 'that' 'bromwell' 'high' 'is' 'far'\n",
      " 'fetched.' 'what' 'a' 'pity' 'that' 'it' 'is' 'not!']\n",
      "['i' 'went' 'and' 'saw' 'this' 'movie' 'last' 'night' 'after' 'being'\n",
      " 'coaxed' 'to' 'by' 'a' 'few' 'friends' 'of' 'mine.' 'I' 'will' 'admit'\n",
      " 'that' 'i' 'was' 'reluctant' 'to' 'see' 'it' 'because' 'from' 'what' 'i'\n",
      " 'knew' 'of' 'ashton' 'kutcher' 'he' 'was' 'only' 'able' 'to' 'do'\n",
      " 'comedy.' 'i' 'was' 'wrong.' 'kutcher' 'played' 'the' 'character' 'of'\n",
      " 'jake' 'fischer' 'very' 'well,' 'and' 'kevin' 'costner' 'played' 'ben'\n",
      " 'randall' 'with' 'such' 'professionalism.' 'the' 'sign' 'of' 'a' 'good'\n",
      " 'movie' 'is' 'that' 'it' 'can' 'toy' 'with' 'our' 'emotions.' 'this'\n",
      " 'one' 'did' 'exactly' 'that.' 'the' 'entire' 'theater' '(which' 'was'\n",
      " 'sold' 'out)' 'was' 'overcome' 'by' 'laughter' 'during' 'the' 'first'\n",
      " 'half' 'of' 'the' 'movie,' 'and' 'were' 'moved' 'to' 'tears' 'during'\n",
      " 'the' 'second' 'half.' 'while' 'exiting' 'the' 'theater' 'i' 'not' 'only'\n",
      " 'saw' 'many' 'women' 'in' 'tears,' 'but' 'many' 'full' 'grown' 'men' 'as'\n",
      " 'well,' 'trying' 'desperately' 'not' 'to' 'let' 'anyone' 'see' 'them'\n",
      " 'crying.' 'this' 'movie' 'was' 'great,' 'and' 'i' 'suggest' 'that' 'you'\n",
      " 'go' 'see' 'it' 'before' 'you' 'judge.']\n"
     ]
    }
   ],
   "source": [
    "print(x_train_preprocessed[0])\n",
    "print(x_test_preprocessed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31a9af",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5351241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "275de466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.str_"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2eb30bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_lexicon(path: str) -> pd.core.frame.DataFrame: \n",
    "    data = pd.read_csv(path, sep='\\t', names=[0, 1, 2, 3])\n",
    "    df = pd.DataFrame()\n",
    "    df['token'] = data[0]\n",
    "    df['sentiment'] = data[1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "853b85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_no_appear(review: np.str_) -> int:\n",
    "    if \"no\" in review:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "755e64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_first_and_second_pro(review: np.str_) -> int:\n",
    "    count = 0\n",
    "    for word in review:\n",
    "        if word in [\"I\", \"i\", \"you\", \"yours\"]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10640c",
   "metadata": {},
   "source": [
    "Let's search for all the words finishing with !\n",
    "\n",
    "As it would make no sense that the ! character appears before the end of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8e86cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_exclamation_appear(review: np.str_) -> int:\n",
    "    if \"!\" in review:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6d0be",
   "metadata": {},
   "source": [
    "it appears no words ends with ! in the training set\n",
    "\n",
    "Thus, there is no need to check for ! in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d10a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_word_count_in_doc(review: np.str_) -> int:\n",
    "    return np.log(len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6ffbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexicon(lexicon: pd.core.frame.DataFrame) -> tuple:\n",
    "    return lexicon[lexicon.sentiment > 0], lexicon[lexicon.sentiment < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6058e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positivity_counter(review: np.str_ , positive_df: pd.core.frame.DataFrame) -> tuple:\n",
    "    posi = np.isin(positive_df.token, review)\n",
    "    #return the # of positive words and their sum\n",
    "    return sum(posi), sum(positive_df.sentiment[posi])\n",
    "\n",
    "def negativity_counter(review : np.str_, negative_df: pd.core.frame.DataFrame) -> tuple:\n",
    "    nega = np.isin(negative_df.token, review)\n",
    "    #return the # of negative words and their sum\n",
    "    return sum(nega), sum(negative_df.sentiment[nega])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc5a4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell' 'high' 'is' 'a' 'cartoon' 'comedy.' 'it' 'ran' 'at' 'the'\n",
      " 'same' 'time' 'as' 'some' 'other' 'programs' 'about' 'school' 'life,'\n",
      " 'such' 'as' '\"teachers\".' 'my' '35' 'years' 'in' 'the' 'teaching'\n",
      " 'profession' 'lead' 'me' 'to' 'believe' 'that' 'bromwell' \"high's\"\n",
      " 'satire' 'is' 'much' 'closer' 'to' 'reality' 'than' 'is' '\"teachers\".'\n",
      " 'the' 'scramble' 'to' 'survive' 'financially,' 'the' 'insightful'\n",
      " 'students' 'who' 'can' 'see' 'right' 'through' 'their' 'pathetic'\n",
      " \"teachers'\" 'pomp,' 'the' 'pettiness' 'of' 'the' 'whole' 'situation,'\n",
      " 'all' 'remind' 'me' 'of' 'the' 'schools' 'i' 'knew' 'and' 'their'\n",
      " 'students.' 'when' 'i' 'saw' 'the' 'episode' 'in' 'which' 'a' 'student'\n",
      " 'repeatedly' 'tried' 'to' 'burn' 'down' 'the' 'school,' 'i' 'immediately'\n",
      " 'recalled' '.........' 'at' '..........' 'high.' 'a' 'classic' 'line:'\n",
      " 'inspector:' 'I' 'am' 'here' 'to' 'sack' 'one' 'of' 'your' 'teachers.'\n",
      " 'student:' 'welcome' 'to' 'bromwell' 'high.' 'i' 'expect' 'that' 'many'\n",
      " 'adults' 'of' 'my' 'age' 'think' 'that' 'bromwell' 'high' 'is' 'far'\n",
      " 'fetched.' 'what' 'a' 'pity' 'that' 'it' 'is' 'not!']\n"
     ]
    }
   ],
   "source": [
    "print(x_train_preprocessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58f8e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoRegression(x_train: list, y_train: list):\n",
    "    lexicon = import_lexicon(\"vader_lexicon.txt\")\n",
    "    positive_df, negative_df = split_lexicon(lexicon)\n",
    "    X_features = []\n",
    "    for review in tqdm(x_train):\n",
    "        feature = np.zeros(8)\n",
    "        feature[0] = does_no_appear(review)\n",
    "        feature[1] = does_exclamation_appear(review)\n",
    "        feature[2] = count_first_and_second_pro(review)\n",
    "        feature[3] = log_word_count_in_doc(review)\n",
    "        feature[4], feature[5] = negativity_counter(review, negative_df)\n",
    "        feature[6], feature[7] = positivity_counter(review, positive_df)\n",
    "        X_features.append(feature)\n",
    "    return np.asarray(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "848e5082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [15:00<00:00, 27.76it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_features = LoRegression(x_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "484e87c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features.shape\n",
    "type(X_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35a65246",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c48cdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [14:47<00:00, 28.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#create X_test using loRegression to have a usable informations\n",
    "X_test_features = LoRegression(x_test_preprocessed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc22a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 8) (25000,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_features)\n",
    "print(X_test_features.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1235a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71532"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c4cbdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "negative review       0.71      0.73      0.72     12500\n",
      "positive review       0.72      0.70      0.71     12500\n",
      "\n",
      "       accuracy                           0.72     25000\n",
      "      macro avg       0.72      0.72      0.72     25000\n",
      "   weighted avg       0.72      0.72      0.72     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['negative review', 'positive review']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534c559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
