{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d906f1",
   "metadata": {},
   "source": [
    "## Installing fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98184e14",
   "metadata": {},
   "source": [
    "The first step of this tutorial is to install and build fastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93382255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 11:54:28--  https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
      "Résolution de github.com (github.com)… 140.82.121.3\n",
      "Connexion à github.com (github.com)|140.82.121.3|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 302 Found\n",
      "Emplacement : https://codeload.github.com/facebookresearch/fastText/zip/v0.9.2 [suivant]\n",
      "--2021-10-06 11:54:29--  https://codeload.github.com/facebookresearch/fastText/zip/v0.9.2\n",
      "Résolution de codeload.github.com (codeload.github.com)… 140.82.121.10\n",
      "Connexion à codeload.github.com (codeload.github.com)|140.82.121.10|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : non indiqué [application/zip]\n",
      "Enregistre : «v0.9.2.zip.1»\n",
      "\n",
      "v0.9.2.zip.1            [    <=>             ]   4,17M  6,90MB/s    ds 0,6s    \n",
      "\n",
      "2021-10-06 11:54:29 (6,90 MB/s) - «v0.9.2.zip.1» enregistré [4369852]\n",
      "\n",
      "Archive:  v0.9.2.zip\n",
      "5b5943c118b0ec5fb9cd8d20587de2b2d3966dfe\n",
      "replace fastText-0.9.2/.circleci/cmake_test.sh? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "make: rien à faire pour « opt ».\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /home/clara/Bureau/EPITA/s9_cours/NLP/nlp_symbolic/fastText-0.9.2\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/clara/.local/lib/python3.8/site-packages (from fasttext==0.9.2) (2.8.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/clara/.local/lib/python3.8/site-packages (from fasttext==0.9.2) (50.3.0)\n",
      "Requirement already satisfied: numpy in /home/clara/.local/lib/python3.8/site-packages (from fasttext==0.9.2) (1.19.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l|"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
    "!unzip v0.9.2.zip\n",
    "!cd fastText-0.9.2 && make && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5d4a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6361f4",
   "metadata": {},
   "source": [
    "### Download directly with command line the english dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4592f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Use --overwrite to download anyway.\r\n"
     ]
    }
   ],
   "source": [
    "!cd fastText-0.9.2 && python3 download_model.py en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdd649",
   "metadata": {},
   "source": [
    "### Import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7fda8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/clara/.local/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (0.0.17)\n",
      "Requirement already satisfied: dill in /home/clara/.local/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: pandas in /home/clara/.local/lib/python3.8/site-packages (from datasets) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: xxhash in /home/clara/.local/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/clara/.local/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: aiohttp in /home/clara/.local/lib/python3.8/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: packaging in /home/clara/.local/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: filelock in /home/clara/.local/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /home/clara/.local/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/clara/.local/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->datasets) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->datasets) (2019.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed519ac1",
   "metadata": {},
   "source": [
    "### Loading the dataset \"IMDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d9236dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345ca03",
   "metadata": {},
   "source": [
    "Using the split argument, we can split the imdb into two separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91a2393f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/token/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/home/token/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('imdb', split='train')\n",
    "test_dataset = load_dataset('imdb', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c693f",
   "metadata": {},
   "source": [
    "*train_dataset* is a class with two attributes :\n",
    "- Features which contains two features : text and label (our x_train and our y_train)\n",
    "- The number of rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9af32b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74d6e0",
   "metadata": {},
   "source": [
    "If we take the first sample of our training dataset, we can see the first review and the label (positive or negative) according to that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b931fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19914e4",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d2f97",
   "metadata": {},
   "source": [
    "With the review we previously saw, the review are definitely not preprocessed. There is still some html tags etc.. \n",
    "We definitely to work on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635f5b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: contractions in /home/token/.local/lib/python3.9/site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/token/.local/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /home/token/.local/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /home/token/.local/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4) (2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/token/.local/lib/python3.9/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /home/token/.local/lib/python3.9/site-packages (from nltk) (2021.8.28)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/token/.local/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /home/token/.local/lib/python3.9/site-packages (from nltk) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install beautifulsoup4\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0da5418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: x\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package biocreative_ppi to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package brown to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/comparative_sentences.zip.\n",
      "       | Downloading package comtrans to /home/token/nltk_data...\n",
      "       | Downloading package conll2000 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to /home/token/nltk_data...\n",
      "       | Downloading package crubadan to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package dolch to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/dolch.zip.\n",
      "       | Downloading package europarl_raw to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/europarl_raw.zip.\n",
      "       | Downloading package floresta to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package framenet_v17 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v17.zip.\n",
      "       | Downloading package gazetteers to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /home/token/nltk_data...\n",
      "       | Downloading package kimmo to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /home/token/nltk_data...\n",
      "       | Downloading package lin_thesaurus to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip.\n",
      "       | Downloading package mac_morpho to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/mac_morpho.zip.\n",
      "       | Downloading package machado to /home/token/nltk_data...\n",
      "       | Downloading package masc_tagged to /home/token/nltk_data...\n",
      "       | Downloading package moses_sample to /home/token/nltk_data...\n",
      "       |   Unzipping models/moses_sample.zip.\n",
      "       | Downloading package movie_reviews to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/movie_reviews.zip.\n",
      "       | Downloading package names to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/names.zip.\n",
      "       | Downloading package nombank.1.0 to /home/token/nltk_data...\n",
      "       | Downloading package nps_chat to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/nps_chat.zip.\n",
      "       | Downloading package omw to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/omw.zip.\n",
      "       | Downloading package opinion_lexicon to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/opinion_lexicon.zip.\n",
      "       | Downloading package paradigms to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/paradigms.zip.\n",
      "       | Downloading package pil to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/pil.zip.\n",
      "       | Downloading package pl196x to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/pl196x.zip.\n",
      "       | Downloading package ppattach to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/ppattach.zip.\n",
      "       | Downloading package problem_reports to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/problem_reports.zip.\n",
      "       | Downloading package propbank to /home/token/nltk_data...\n",
      "       | Downloading package ptb to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/ptb.zip.\n",
      "       | Downloading package product_reviews_1 to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_1.zip.\n",
      "       | Downloading package product_reviews_2 to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_2.zip.\n",
      "       | Downloading package pros_cons to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/pros_cons.zip.\n",
      "       | Downloading package qc to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/qc.zip.\n",
      "       | Downloading package reuters to /home/token/nltk_data...\n",
      "       | Downloading package rte to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/rte.zip.\n",
      "       | Downloading package semcor to /home/token/nltk_data...\n",
      "       | Downloading package senseval to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/senseval.zip.\n",
      "       | Downloading package sentiwordnet to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/sentiwordnet.zip.\n",
      "       | Downloading package sentence_polarity to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/sentence_polarity.zip.\n",
      "       | Downloading package shakespeare to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/shakespeare.zip.\n",
      "       | Downloading package sinica_treebank to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/sinica_treebank.zip.\n",
      "       | Downloading package smultron to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/smultron.zip.\n",
      "       | Downloading package state_union to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/state_union.zip.\n",
      "       | Downloading package stopwords to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/stopwords.zip.\n",
      "       | Downloading package subjectivity to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/subjectivity.zip.\n",
      "       | Downloading package swadesh to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/swadesh.zip.\n",
      "       | Downloading package switchboard to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/switchboard.zip.\n",
      "       | Downloading package timit to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/timit.zip.\n",
      "       | Downloading package toolbox to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/toolbox.zip.\n",
      "       | Downloading package treebank to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/treebank.zip.\n",
      "       | Downloading package twitter_samples to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/twitter_samples.zip.\n",
      "       | Downloading package udhr to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/udhr.zip.\n",
      "       | Downloading package udhr2 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/udhr2.zip.\n",
      "       | Downloading package unicode_samples to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/unicode_samples.zip.\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /home/token/nltk_data...\n",
      "       | Downloading package verbnet to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/verbnet.zip.\n",
      "       | Downloading package verbnet3 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/verbnet3.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       | Downloading package webtext to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/webtext.zip.\n",
      "       | Downloading package wordnet to /home/token/nltk_data...\n",
      "       |   Package wordnet is already up-to-date!\n",
      "       | Downloading package wordnet_ic to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/wordnet_ic.zip.\n",
      "       | Downloading package words to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/words.zip.\n",
      "       | Downloading package ycoe to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/ycoe.zip.\n",
      "       | Downloading package rslp to /home/token/nltk_data...\n",
      "       |   Unzipping stemmers/rslp.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package universal_tagset to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping taggers/universal_tagset.zip.\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "       | Downloading package punkt to /home/token/nltk_data...\n",
      "       |   Package punkt is already up-to-date!\n",
      "       | Downloading package book_grammars to /home/token/nltk_data...\n",
      "       |   Unzipping grammars/book_grammars.zip.\n",
      "       | Downloading package sample_grammars to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping grammars/sample_grammars.zip.\n",
      "       | Downloading package spanish_grammars to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping grammars/spanish_grammars.zip.\n",
      "       | Downloading package basque_grammars to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping grammars/basque_grammars.zip.\n",
      "       | Downloading package large_grammars to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping grammars/large_grammars.zip.\n",
      "       | Downloading package tagsets to /home/token/nltk_data...\n",
      "       |   Unzipping help/tagsets.zip.\n",
      "       | Downloading package snowball_data to /home/token/nltk_data...\n",
      "       | Downloading package bllip_wsj_no_aux to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "       | Downloading package word2vec_sample to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping models/word2vec_sample.zip.\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     /home/token/nltk_data...\n",
      "       | Downloading package mte_teip5 to /home/token/nltk_data...\n",
      "       |   Unzipping corpora/mte_teip5.zip.\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "       | Downloading package averaged_perceptron_tagger_ru to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
      "       | Downloading package perluniprops to /home/token/nltk_data...\n",
      "       |   Unzipping misc/perluniprops.zip.\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /home/token/nltk_data...\n",
      "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "       | Downloading package vader_lexicon to /home/token/nltk_data...\n",
      "       | Downloading package porter_test to /home/token/nltk_data...\n",
      "       |   Unzipping stemmers/porter_test.zip.\n",
      "       | Downloading package wmt15_eval to /home/token/nltk_data...\n",
      "       |   Unzipping models/wmt15_eval.zip.\n",
      "       | Downloading package mwa_ppdb to /home/token/nltk_data...\n",
      "       |   Unzipping misc/mwa_ppdb.zip.\n",
      "       | \n",
      "     Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('all')\n",
    "\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text: str) -> str:\n",
    "    '''\n",
    "    Strip html tags from the input text.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    stripped_text (str): Text completely stripped without html tags.\n",
    "    '''\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e08798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text: str) -> str:\n",
    "    '''\n",
    "    Remove accented characters from the text.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text which does not contains accented characters.\n",
    "    '''\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f97db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    '''\n",
    "    Expand contractions like \"you're\" to \"you are\".\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text with no contractions.\n",
    "    '''\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    '''\n",
    "    Expand contractions like \"you're\" to \"you are\".\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text with no contractions.\n",
    "    '''\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text: str , remove_digits: bool) -> str:\n",
    "    '''\n",
    "    Remove special characters from the text.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    remove_digits (bool): True if you want to remove digits, else False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text which does not contains special characters (digits, etc...).\n",
    "    '''\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c593691",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemming(text: str, lemming: bool) -> str:\n",
    "    '''\n",
    "    The words in our text input will be lemmatized to their root form. \n",
    "    Furthermore, the stop words and the words with the length less than 4 will be removed from the corpus.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    lemming (bool): True if you want the lemmitization, if not False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Lemmed text.\n",
    "    '''\n",
    "    \n",
    "    if (lemming):\n",
    "        tokens = text.split()\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ead55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming(text: str, stemming: bool) -> str:\n",
    "    '''\n",
    "    Stemming is the process of producing morphological variants of a root/base word.\n",
    "    We will use this function to do the same for our document.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    lemming (bool): True if you want the stemmatization, if not False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Stemmed text.\n",
    "    '''\n",
    "    if (stemming):\n",
    "        tokens = text.split()\n",
    "        tokens = [ps.stem(word) for word in tokens]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e8788ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def stop_words(text: str, stop_words: bool) -> str:\n",
    "    '''\n",
    "    ????\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    stop_words (bool): True if you want the delete them, if not False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text without stop words.\n",
    "    '''\n",
    "    if (stop_words):\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e602a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_document(document: str, lemm: bool, stemm: bool, sw: bool) -> str:\n",
    "    '''\n",
    "    Preprocess the document to remove all the unnecessary information.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text pre-processed.\n",
    "    '''\n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, True)  \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    document = lemming(document, lemm)\n",
    "    document = stemming(document, stemm)\n",
    "    \n",
    "    document = stop_words(document, sw)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "for i in range(train_dataset.num_rows):\n",
    "    x_train.append(pre_process_document(train_dataset[i]['text'], False, False, False))\n",
    "    \n",
    "\n",
    "for i in range(test_dataset.num_rows):\n",
    "    x_test.append(pre_process_document(test_dataset[i]['text'], False, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba2262ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = train_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c142f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f74d686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imdb_train_file(train_dataset, y_train: list,\n",
    "                           lemm: bool = False, stemm:bool = False, stop_words:bool = False):\n",
    "    '''\n",
    "    Create the imdb train file according to the fasttext documentation.\n",
    "    \n",
    "             Parameters:\n",
    "                     train_dataset: Training dataset\n",
    "                     y_train : Label for the training dataset\n",
    "    '''\n",
    "    \n",
    "    if (lemm):\n",
    "        f = open(\"imdb_train_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_train_stemmed.txt\", \"w\")\n",
    "        \n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_train_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_train.txt\", \"w\") \n",
    "    \n",
    "    for i in tqdm(range(train_dataset.num_rows)):\n",
    "        if y_train[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_train[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3660bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imdb_test_file(test_dataset, y_test: list, lemm: bool = False, stemm: bool = False, stop_words: bool = False):\n",
    "    '''\n",
    "    Create the imdb test file according to the fasttext documentation.\n",
    "    \n",
    "             Parameters:\n",
    "                     test_dataset: Training dataset\n",
    "                     y_test: Label for the testing dataset\n",
    "    '''\n",
    "    if (lemm):\n",
    "        f = open(\"imdb_test_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_test_stemmed.txt\", \"w\")\n",
    "\n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_test_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_test.txt\", \"w\") \n",
    "    \n",
    "    for i in tqdm(range(test_dataset.num_rows)):\n",
    "        if y_test[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(test_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_test[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(test_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b2643aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imdb_train_and_validation_file(train_dataset, y_train: list, lemm: bool = False,\n",
    "                                          stemm: bool = False, stop_words: bool = False):\n",
    "    '''\n",
    "    Create the imdb validation file according to the fasttext documentation.\n",
    "    \n",
    "             Parameters:\n",
    "                     train_dataset: Training dataset\n",
    "                     y_train: Label for the training dataset\n",
    "    '''\n",
    "    i = 0\n",
    "    \n",
    "    if (lemm):\n",
    "        f = open(\"imdb_validation_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_validation_stemmed.txt\", \"w\")\n",
    "\n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_validation_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_validation.txt\", \"w\") \n",
    "    \n",
    "    for i in tqdm(range(5000)):\n",
    "        if y_train[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_train[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    if (lemm):\n",
    "        f = open(\"imdb_train_splited_with_the_validation_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_train_splited_with_the_validation_stemmed.txt\", \"w\")\n",
    "\n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_train_splited_with_the_validation_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_train_splited_with_the_validation.txt\", \"w\") \n",
    "    \n",
    "    while i != train_dataset.num_rows:\n",
    "        if y_train[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_train[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        i += 1\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fabae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:13<00:00, 1796.02it/s]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 1864.51it/s]\n",
      "100%|██████████| 25000/25000 [00:13<00:00, 1812.98it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train)\n",
    "create_imdb_test_file(test_dataset, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae6571",
   "metadata": {},
   "source": [
    "## Let's train the model using fasttext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06dc732",
   "metadata": {},
   "source": [
    "##### 1) Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37becb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:32<00:00, 761.64it/s]\n",
      "100%|██████████| 5000/5000 [00:06<00:00, 762.32it/s]\n",
      "100%|██████████| 25000/25000 [00:33<00:00, 744.22it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train, lemm = True)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train, lemm = True)\n",
    "create_imdb_test_file(test_dataset, y_test, lemm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5ad2c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 183 ms, total: 21.6 s\n",
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_lemmed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8606f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.8726, Rappel: 0.8726\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84519c5",
   "metadata": {},
   "source": [
    "##### 2) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ecd6ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:36<00:00, 258.92it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 253.94it/s]\n",
      "100%|██████████| 25000/25000 [01:36<00:00, 259.78it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train, stemm = True)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train, stemm = True)\n",
    "create_imdb_test_file(test_dataset, y_test, stemm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "946f812e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 167 ms, total: 20.4 s\n",
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_stemmed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56ff767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.8164, Rappel: 0.8164\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bdecd",
   "metadata": {},
   "source": [
    "##### 3) Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d2ab867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:17<00:00, 1451.18it/s]\n",
      "100%|██████████| 5000/5000 [00:03<00:00, 1533.33it/s]\n",
      "100%|██████████| 25000/25000 [00:15<00:00, 1569.33it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train, stop_words = True)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train, stop_words = True)\n",
    "create_imdb_test_file(test_dataset, y_test, stop_words = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf9d36d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.91 s, sys: 183 ms, total: 10.1 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_sw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "86ddd80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.87496, Rappel: 0.87496\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d67c5",
   "metadata": {},
   "source": [
    "##### 4) None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a00c7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 276 ms, total: 18.8 s\n",
      "Wall time: 3.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e14b22da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.87548, Rappel: 0.87548\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1125b05",
   "metadata": {},
   "source": [
    "### Making the model better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbc9b2",
   "metadata": {},
   "source": [
    "#### How about N-grams ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4da0992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.2 s, sys: 542 ms, total: 43.8 s\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", wordNgrams=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a638f1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.86576, Rappel: 0.86576\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717628d",
   "metadata": {},
   "source": [
    "#### With more epochs and a smaller learning_rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b3d63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 1.61 s, total: 3min 33s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", lr=0.5, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e288098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.89536, Rappel: 0.89536\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c0fa5",
   "metadata": {},
   "source": [
    "#### With more epochs and a smaller smaller learning_rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "111bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1.91 s, total: 3min 35s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", lr=0.25, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "859e6bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.89628, Rappel: 0.89628\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b94f6d",
   "metadata": {},
   "source": [
    "#### With more epochs and a bigger learning_rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94e07a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 31s, sys: 1.7 s, total: 3min 33s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", lr=1.0, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "86e20431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.8952, Rappel: 0.8952\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841f889",
   "metadata": {},
   "source": [
    "#### With the autotuneValidationfile (testing and performance purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bea032c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 48s, sys: 12.5 s, total: 26min\n",
      "Wall time: 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_splited_with_the_validation.txt\", autotuneValidationFile='imdb_validation.txt', autotuneDuration=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "567ab579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sample: 25000, Taux de précision: 0.87896, Rappel: 0.87896\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(\"Nombre de sample: \" + str(res[0]) + \", Taux de précision: \" + str(res[1]) + \", Rappel: \" + str(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6a7da",
   "metadata": {},
   "source": [
    "## Beating the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2199f2d",
   "metadata": {},
   "source": [
    "###### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "397feeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: zeugma in /home/token/.local/lib/python3.9/site-packages (0.49)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (1.2.4)\n",
      "Requirement already satisfied: keras>=2.1.3 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (2.6.0)\n",
      "Requirement already satisfied: Cython>=0.27.3 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (0.29.24)\n",
      "Requirement already satisfied: tensorflow>=1.5.0 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (1.19.5)\n",
      "Requirement already satisfied: gensim>=3.5.0 in /home/token/.local/lib/python3.9/site-packages (from zeugma) (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/token/.local/lib/python3.9/site-packages (from gensim>=3.5.0->zeugma) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/token/.local/lib/python3.9/site-packages (from gensim>=3.5.0->zeugma) (1.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas>=0.20.3->zeugma) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=0.20.3->zeugma) (2021.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/token/.local/lib/python3.9/site-packages (from scikit-learn>=0.19.1->zeugma) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/token/.local/lib/python3.9/site-packages (from scikit-learn>=0.19.1->zeugma) (2.1.0)\n",
      "Requirement already satisfied: clang~=5.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow>=1.5.0->zeugma) (3.12.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (1.12.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (1.38.1)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/lib/python3/dist-packages (from tensorflow>=1.5.0->zeugma) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (0.2.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (0.4.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (0.37.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/token/.local/lib/python3.9/site-packages (from tensorflow>=1.5.0->zeugma) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/token/.local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/token/.local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (1.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (52.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/token/.local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/token/.local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/token/.local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/token/.local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/token/.local/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/token/.local/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/token/.local/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/token/.local/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/token/.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=1.5.0->zeugma) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install zeugma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b7acdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75213ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = EmbeddingTransformer('glove')\n",
    "x_train = glove.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "647839a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.76     12500\n",
      "           1       0.76      0.73      0.75     12500\n",
      "\n",
      "    accuracy                           0.75     25000\n",
      "   macro avg       0.75      0.75      0.75     25000\n",
      "weighted avg       0.75      0.75      0.75     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "x_test = glove.transform(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
