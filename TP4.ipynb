{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d906f1",
   "metadata": {},
   "source": [
    "## Installing fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98184e14",
   "metadata": {},
   "source": [
    "The first step of this tutorial is to install and build fastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93382255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 11:54:28--  https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
      "Résolution de github.com (github.com)… 140.82.121.3\n",
      "Connexion à github.com (github.com)|140.82.121.3|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 302 Found\n",
      "Emplacement : https://codeload.github.com/facebookresearch/fastText/zip/v0.9.2 [suivant]\n",
      "--2021-10-06 11:54:29--  https://codeload.github.com/facebookresearch/fastText/zip/v0.9.2\n",
      "Résolution de codeload.github.com (codeload.github.com)… 140.82.121.10\n",
      "Connexion à codeload.github.com (codeload.github.com)|140.82.121.10|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : non indiqué [application/zip]\n",
      "Enregistre : «v0.9.2.zip.1»\n",
      "\n",
      "v0.9.2.zip.1            [    <=>             ]   4,17M  6,90MB/s    ds 0,6s    \n",
      "\n",
      "2021-10-06 11:54:29 (6,90 MB/s) - «v0.9.2.zip.1» enregistré [4369852]\n",
      "\n",
      "Archive:  v0.9.2.zip\n",
      "5b5943c118b0ec5fb9cd8d20587de2b2d3966dfe\n",
      "replace fastText-0.9.2/.circleci/cmake_test.sh? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "make: rien à faire pour « opt ».\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /home/clara/Bureau/EPITA/s9_cours/NLP/nlp_symbolic/fastText-0.9.2\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/clara/.local/lib/python3.8/site-packages (from fasttext==0.9.2) (2.8.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/clara/.local/lib/python3.8/site-packages (from fasttext==0.9.2) (50.3.0)\n",
      "Requirement already satisfied: numpy in /home/clara/.local/lib/python3.8/site-packages (from fasttext==0.9.2) (1.19.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l|"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
    "!unzip v0.9.2.zip\n",
    "!cd fastText-0.9.2 && make && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d4a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6361f4",
   "metadata": {},
   "source": [
    "### Download directly with command line the english dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4592f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Use --overwrite to download anyway.\r\n"
     ]
    }
   ],
   "source": [
    "!cd fastText-0.9.2 && python3 download_model.py en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdd649",
   "metadata": {},
   "source": [
    "### Import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7fda8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/clara/.local/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (0.0.17)\n",
      "Requirement already satisfied: dill in /home/clara/.local/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: pandas in /home/clara/.local/lib/python3.8/site-packages (from datasets) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: xxhash in /home/clara/.local/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/clara/.local/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: aiohttp in /home/clara/.local/lib/python3.8/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/clara/.local/lib/python3.8/site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: packaging in /home/clara/.local/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: filelock in /home/clara/.local/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /home/clara/.local/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/clara/.local/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/clara/.local/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->datasets) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->datasets) (2019.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed519ac1",
   "metadata": {},
   "source": [
    "### Loading the dataset \"IMDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9236dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345ca03",
   "metadata": {},
   "source": [
    "Using the split argument, we can split the imdb into two separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a2393f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/clara/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/home/clara/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('imdb', split='train')\n",
    "test_dataset = load_dataset('imdb', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c693f",
   "metadata": {},
   "source": [
    "*train_dataset* is a class with two attributes :\n",
    "- Features which contains two features : text and label (our x_train and our y_train)\n",
    "- The number of rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9af32b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74d6e0",
   "metadata": {},
   "source": [
    "If we take the first sample of our training dataset, we can see the first review and the label (positive or negative) according to that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b931fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19914e4",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d2f97",
   "metadata": {},
   "source": [
    "With the review we previously saw, the review are definitely not preprocessed. There is still some html tags etc.. \n",
    "We definitely to work on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "635f5b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: contractions in /home/clara/.local/lib/python3.8/site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/clara/.local/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /home/clara/.local/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /home/clara/.local/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /home/clara/.local/lib/python3.8/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/clara/.local/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/clara/.local/lib/python3.8/site-packages (3.6.3)\n",
      "Requirement already satisfied: tqdm in /home/clara/.local/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in /home/clara/.local/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /home/clara/.local/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex in /home/clara/.local/lib/python3.8/site-packages (from nltk) (2021.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install beautifulsoup4\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0da5418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "700e9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text: str) -> str:\n",
    "    '''\n",
    "    Strip html tags from the input text.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    stripped_text (str): Text completely stripped without html tags.\n",
    "    '''\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16e08798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text: str) -> str:\n",
    "    '''\n",
    "    Remove accented characters from the text.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text which does not contains accented characters.\n",
    "    '''\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "26f97db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    '''\n",
    "    Expand contractions like \"you're\" to \"you are\".\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text with no contractions.\n",
    "    '''\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6c3f72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    '''\n",
    "    Expand contractions like \"you're\" to \"you are\".\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text with no contractions.\n",
    "    '''\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c99d7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text: str , remove_digits: bool) -> str:\n",
    "    '''\n",
    "    Remove special characters from the text.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    remove_digits (bool): True if you want to remove digits, else False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text which does not contains special characters (digits, etc...).\n",
    "    '''\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2c593691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/clara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemming(text: str, lemming: bool) -> str:\n",
    "    '''\n",
    "    The words in our text input will be lemmatized to their root form. \n",
    "    Furthermore, the stop words and the words with the length less than 4 will be removed from the corpus.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    lemming (bool): True if you want the lemmitization, if not False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Lemmed text.\n",
    "    '''\n",
    "    \n",
    "    if (lemming):\n",
    "        tokens = text.split()\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c23ead55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming(text: str, stemming: bool) -> str:\n",
    "    '''\n",
    "    Stemming is the process of producing morphological variants of a root/base word.\n",
    "    We will use this function to do the same for our document.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    lemming (bool): True if you want the stemmatization, if not False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Stemmed text.\n",
    "    '''\n",
    "    if (stemming):\n",
    "        tokens = text.split()\n",
    "        tokens = [ps.stem(word) for word in tokens]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e8788ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def stop_words(text: str, stop_words: bool) -> str:\n",
    "    '''\n",
    "    ????\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "                    stop_words (bool): True if you want the delete them, if not False.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text without stop words.\n",
    "    '''\n",
    "    if (stop_words):\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e602a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_document(document: str, lemm: bool, stemm: bool, sw: bool) -> str:\n",
    "    '''\n",
    "    Preprocess the document to remove all the unnecessary information.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): A text.\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Text pre-processed.\n",
    "    '''\n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, True)  \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    document = lemming(document, lemm)\n",
    "    document = stemming(document, stemm)\n",
    "    \n",
    "    document = stop_words(document, sw)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba2262ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = train_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c142f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f74d686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imdb_train_file(train_dataset, y_train: list,\n",
    "                           lemm: bool = False, stemm:bool = False, stop_words:bool = False):\n",
    "    '''\n",
    "    Create the imdb train file according to the fasttext documentation.\n",
    "    \n",
    "             Parameters:\n",
    "                     train_dataset: Training dataset\n",
    "                     y_train : Label for the training dataset\n",
    "    '''\n",
    "    \n",
    "    if (lemm):\n",
    "        f = open(\"imdb_train_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_train_stemmed.txt\", \"w\")\n",
    "        \n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_train_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_train.txt\", \"w\") \n",
    "    \n",
    "    for i in tqdm(range(train_dataset.num_rows)):\n",
    "        if y_train[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_train[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3660bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imdb_test_file(test_dataset, y_test: list, lemm: bool = False, stemm: bool = False, stop_words: bool = False):\n",
    "    '''\n",
    "    Create the imdb test file according to the fasttext documentation.\n",
    "    \n",
    "             Parameters:\n",
    "                     test_dataset: Training dataset\n",
    "                     y_test: Label for the testing dataset\n",
    "    '''\n",
    "    if (lemm):\n",
    "        f = open(\"imdb_test_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_test_stemmed.txt\", \"w\")\n",
    "\n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_test_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_test.txt\", \"w\") \n",
    "    \n",
    "    for i in tqdm(range(test_dataset.num_rows)):\n",
    "        if y_test[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(test_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_test[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(test_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8b2643aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imdb_train_and_validation_file(train_dataset, y_train: list, lemm: bool = False,\n",
    "                                          stemm: bool = False, stop_words: bool = False):\n",
    "    '''\n",
    "    Create the imdb validation file according to the fasttext documentation.\n",
    "    \n",
    "             Parameters:\n",
    "                     train_dataset: Training dataset\n",
    "                     y_train: Label for the training dataset\n",
    "    '''\n",
    "    i = 0\n",
    "    \n",
    "    if (lemm):\n",
    "        f = open(\"imdb_validation_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_validation_stemmed.txt\", \"w\")\n",
    "\n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_validation_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_validation.txt\", \"w\") \n",
    "    \n",
    "    for i in tqdm(range(5000)):\n",
    "        if y_train[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_train[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    if (lemm):\n",
    "        f = open(\"imdb_train_splited_with_the_validation_lemmed.txt\", \"w\")\n",
    "    \n",
    "    elif (stemm):\n",
    "        f = open(\"imdb_train_splited_with_the_validation_stemmed.txt\", \"w\")\n",
    "\n",
    "    elif (stop_words):\n",
    "        f = open(\"imdb_train_splited_with_the_validation_sw.txt\", \"w\")        \n",
    "\n",
    "    else:\n",
    "        f = open(\"imdb_train_splited_with_the_validation.txt\", \"w\") \n",
    "    \n",
    "    while i != train_dataset.num_rows:\n",
    "        if y_train[i] == 1:\n",
    "            f.write(\"__label__positive\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        if y_train[i] == 0:\n",
    "            f.write(\"__label__negative\" + \" \" + pre_process_document(train_dataset[i]['text'], lemm, stemm, stop_words) + '\\n')\n",
    "        i += 1\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6fabae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:19<00:00, 1281.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1388.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:18<00:00, 1352.43it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train)\n",
    "create_imdb_test_file(test_dataset, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae6571",
   "metadata": {},
   "source": [
    "## Let's train the model using fasttext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06dc732",
   "metadata": {},
   "source": [
    "##### 1) Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37becb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:43<00:00, 574.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:08<00:00, 571.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:44<00:00, 564.23it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train, lemm = True)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train, lemm = True)\n",
    "create_imdb_test_file(test_dataset, y_test, lemm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5ad2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  93399\n",
      "Number of labels: 2\n",
      "Progress:  99.2% words/sec/thread: 2255284 lr:  0.000769 avg.loss:  0.403260 ETA:   0h 0m 0s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 249 ms, total: 12.7 s\n",
      "Wall time: 5.33 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread: 2221208 lr: -0.000003 avg.loss:  0.404254 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 2221161 lr:  0.000000 avg.loss:  0.404254 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_lemmed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8606f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 0.84256, 0.84256)\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84519c5",
   "metadata": {},
   "source": [
    "##### 2) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ecd6ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [01:52<00:00, 221.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:21<00:00, 233.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [01:47<00:00, 232.38it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train, stemm = True)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train, stemm = True)\n",
    "create_imdb_test_file(test_dataset, y_test, stemm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "946f812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  74057\n",
      "Number of labels: 2\n",
      "Progress:  98.4% words/sec/thread: 2469537 lr:  0.001571 avg.loss:  0.264267 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 224 ms, total: 11.8 s\n",
      "Wall time: 4.61 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 100.0% words/sec/thread: 2446274 lr: -0.000002 avg.loss:  0.263788 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 2446215 lr:  0.000000 avg.loss:  0.263788 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_stemmed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56ff767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 0.81796, 0.81796)\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bdecd",
   "metadata": {},
   "source": [
    "##### 3) Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d2ab867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:21<00:00, 1179.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1293.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:19<00:00, 1289.47it/s]\n"
     ]
    }
   ],
   "source": [
    "create_imdb_train_file(train_dataset, y_train, stop_words = True)\n",
    "create_imdb_train_and_validation_file(train_dataset, y_train, stop_words = True)\n",
    "create_imdb_test_file(test_dataset, y_test, stop_words = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bf9d36d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  99724\n",
      "Number of labels: 2\n",
      "Progress:  94.0% words/sec/thread: 1949998 lr:  0.006038 avg.loss:  0.248016 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.98 s, sys: 163 ms, total: 7.14 s\n",
      "Wall time: 2.93 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:  98.5% words/sec/thread: 1954389 lr:  0.001543 avg.loss:  0.243901 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 1902308 lr: -0.000003 avg.loss:  0.242394 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 1902247 lr:  0.000000 avg.loss:  0.242394 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_sw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "86ddd80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 0.87176, 0.87176)\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d67c5",
   "metadata": {},
   "source": [
    "##### 4) None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a00c7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  102284\n",
      "Number of labels: 2\n",
      "Progress:  95.2% words/sec/thread: 2450738 lr:  0.004787 avg.loss:  0.298446 ETA:   0h 0m 0s 0.015025 avg.loss:  0.318316 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 179 ms, total: 12.1 s\n",
      "Wall time: 4.73 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:  98.0% words/sec/thread: 2456976 lr:  0.002008 avg.loss:  0.295846 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 2444701 lr: -0.000002 avg.loss:  0.294353 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 2444660 lr:  0.000000 avg.loss:  0.294353 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e14b22da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 0.84588, 0.84588)\n"
     ]
    }
   ],
   "source": [
    "res = model.test(\"imdb_test.txt\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1125b05",
   "metadata": {},
   "source": [
    "### Making the model better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbc9b2",
   "metadata": {},
   "source": [
    "#### How about N-grams ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4da0992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  102284\n",
      "Number of labels: 2\n",
      "Progress:  98.3% words/sec/thread:  952230 lr:  0.001678 avg.loss:  0.403023 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 516 ms, total: 31.3 s\n",
      "Wall time: 11.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:  99.2% words/sec/thread:  951587 lr:  0.000772 avg.loss:  0.401473 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  949701 lr: -0.000002 avg.loss:  0.400764 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  949692 lr:  0.000000 avg.loss:  0.400764 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", wordNgrams=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a638f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.86824, 0.86824)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"imdb_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717628d",
   "metadata": {},
   "source": [
    "#### With more epochs and a smaller learning_rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b3d63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  102284\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  884986 lr:  0.000275 avg.loss:  0.054940 ETA:   0h 0m 0s 14.0% words/sec/thread:  937863 lr:  0.430021 avg.loss:  0.283242 ETA:   0h 0m44sm43s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 2 s, total: 2min 34s\n",
      "Wall time: 56.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " words/sec/thread:  883877 lr: -0.000002 avg.loss:  0.054908 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  883876 lr:  0.000000 avg.loss:  0.054908 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", lr=0.5, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e288098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.89544, 0.89544)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"imdb_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c0fa5",
   "metadata": {},
   "source": [
    "#### With more epochs and a smaller smaller learning_rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "111bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  102284\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  925738 lr:  0.000071 avg.loss:  0.054127 ETA:   0h 0m 0s 30.7% words/sec/thread:  903140 lr:  0.173331 avg.loss:  0.152286 ETA:   0h 0m37s 0.113013 avg.loss:  0.093962 ETA:   0h 0m24s words/sec/thread:  915850 lr:  0.085151 avg.loss:  0.079338 ETA:   0h 0m18sm12s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 29s, sys: 1.94 s, total: 2min 31s\n",
      "Wall time: 54.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% words/sec/thread:  924256 lr: -0.000001 avg.loss:  0.054114 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  924255 lr:  0.000000 avg.loss:  0.054114 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", lr=0.25, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "859e6bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.89024, 0.89024)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"imdb_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b94f6d",
   "metadata": {},
   "source": [
    "#### With more epochs and a bigger learning_rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "94e07a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  102284\n",
      "Number of labels: 2\n",
      "Progress:  99.8% words/sec/thread:  922500 lr:  0.001503 avg.loss:  0.042932 ETA:   0h 0m 0s 23.9% words/sec/thread:  906174 lr:  0.760812 avg.loss:  0.163926 ETA:   0h 0m41s 72.2% words/sec/thread:  914289 lr:  0.277827 avg.loss:  0.058940 ETA:   0h 0m14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 29s, sys: 1.85 s, total: 2min 31s\n",
      "Wall time: 54.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% words/sec/thread:  922152 lr: -0.000005 avg.loss:  0.042871 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  922149 lr:  0.000000 avg.loss:  0.042871 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train.txt\", lr=1.0, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "86e20431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.8914, 0.8914)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"imdb_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841f889",
   "metadata": {},
   "source": [
    "#### With the autotuneValidationfile (testing and performance purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bea032c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.921800 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  90597\n",
      "Number of labels: 2\n",
      "Progress:  95.4% words/sec/thread: 1806095 lr:  0.008937 avg.loss:  0.388595 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 47s, sys: 7.62 s, total: 13min 54s\n",
      "Wall time: 5min 7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:  98.1% words/sec/thread: 1802697 lr:  0.003644 avg.loss:  0.386951 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 1784475 lr: -0.000008 avg.loss:  0.386236 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread: 1784449 lr:  0.000000 avg.loss:  0.386236 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"imdb_train_splited_with_the_validation.txt\", autotuneValidationFile='imdb_validation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "567ab579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.85736, 0.85736)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"imdb_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6a7da",
   "metadata": {},
   "source": [
    "## Beating the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b630d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf27532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
